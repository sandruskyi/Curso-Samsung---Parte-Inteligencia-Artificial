{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Modulo4__Api.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHBoFzPvoTlK",
        "colab_type": "text"
      },
      "source": [
        "### **Modulo 4 Apis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfDUc61PoTlL",
        "colab_type": "text"
      },
      "source": [
        "Adquisición de datos en Python\n",
        "------------------------------------------------------\n",
        "\n",
        "## Instrucciones de uso\n",
        "\n",
        "A continuación se presentarán explicaciones y ejemplos de adquisición de datos en Python. Recordad que podéis ir ejecutando los ejemplos para obtener sus resultados.\n",
        "\n",
        "## Introducción\n",
        "\n",
        "Los procesos de adquisición de datos son muy diversos. En esta unidad, veremos ejemplos de adquisición de datos de Internet con tres métodos diferentes:\n",
        "\n",
        "- descarga directa\n",
        "- petición a APIs de terceros\n",
        "- *web crawling*\n",
        "\n",
        "Por lo que respeta a la interacción con APIs de terceros, repasaremos dos alternativas, la construcción manual de las peticiones HTTP y el uso de librerías Python. \n",
        "\n",
        "Con relación al *web crawling*, veremos cómo utilizar la librería [Scrapy](https://scrapy.org/) para construir un pequeño *web crawler* que capture datos de nuestro interés. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLPs2K0MoTlM",
        "colab_type": "text"
      },
      "source": [
        "### Primeros pasos\n",
        "\n",
        "En esta unidad trabajaremos en varias ocasiones con datos en formato JSON (recordad que ya hemos introducido el formato JSON en la xwiki). \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILeYBheUoTlM",
        "colab_type": "text"
      },
      "source": [
        "La librería json de Python nos ofrece algunas funciones muy útiles para trabajar en este formato. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDI3oLHpoTlN",
        "colab_type": "code",
        "outputId": "2d0080bd-d835-4c25-81a0-f554e6c8707c",
        "colab": {}
      },
      "source": [
        "# Construimos un diccionario de ejemplo y mostramos el tipo de datos y el contenido de la variable.\n",
        "diccionario_ejemplo = {\"nombre\": \"Yann\", \"apellidos\": {\"apellido1\": \"LeCun\", \"apellido2\": \"-\"}, \"edad\": 56}\n",
        "print(type(diccionario_ejemplo))\n",
        "print(diccionario_ejemplo)\n",
        "\n",
        "# Construimos una lista de ejemplo y mostramos el tipo de datos y el contenido de la variable.\n",
        "lista_ejemplo = [1, 2, 3]\n",
        "print(type(lista_ejemplo))\n",
        "print(lista_ejemplo)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'dict'>\n",
            "{'nombre': 'Yann', 'apellidos': {'apellido1': 'LeCun', 'apellido2': '-'}, 'edad': 56}\n",
            "<class 'list'>\n",
            "[1, 2, 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Vw-k8R0oTlS",
        "colab_type": "code",
        "outputId": "1f40fbd1-936e-42d7-f347-f760151356f5",
        "colab": {}
      },
      "source": [
        "# Importamos la librería json.\n",
        "import json\n",
        "\n",
        "# Mostramos la representación json del diccionario.\n",
        "json_dict = json.dumps(diccionario_ejemplo)\n",
        "print(type(json_dict))\n",
        "print(json_dict)\n",
        "\n",
        "# Mostramos la representación json de la lista.\n",
        "json_list = json.dumps(lista_ejemplo)\n",
        "print(type(json_list))\n",
        "print(json_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'str'>\n",
            "{\"nombre\": \"Yann\", \"apellidos\": {\"apellido1\": \"LeCun\", \"apellido2\": \"-\"}, \"edad\": 56}\n",
            "<class 'str'>\n",
            "[1, 2, 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22_JQ41toTlU",
        "colab_type": "text"
      },
      "source": [
        "Fijaos que, en ambos casos, obtenemos una cadena de caracteres que nos representa, en formato JSON, los objetos Python. Este proceso se conoce como **serializar** el objeto.\n",
        "\n",
        "También podemos realizar el proceso inverso (conocido como **deserializar**), creando objetos Python (por ejemplo, listas o diccionarios) a partir de cadenas de texto en formato JSON.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bjpjE8LoTlV",
        "colab_type": "code",
        "outputId": "9ca61a90-795b-4241-d396-da9be7bb802b",
        "colab": {}
      },
      "source": [
        "# Deserializamos la cadena json_dict.\n",
        "diccionario_ejemplo2 = json.loads(json_dict)\n",
        "print(type(diccionario_ejemplo2))\n",
        "print(diccionario_ejemplo2)\n",
        "\n",
        "# Deserializamos la cadena json_list.\n",
        "lista_ejemplo2 = json.loads(json_list)\n",
        "print(type(lista_ejemplo2))\n",
        "print(lista_ejemplo2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'dict'>\n",
            "{'nombre': 'Yann', 'apellidos': {'apellido1': 'LeCun', 'apellido2': '-'}, 'edad': 56}\n",
            "<class 'list'>\n",
            "[1, 2, 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka5E1yALoTlZ",
        "colab_type": "text"
      },
      "source": [
        "Para mejorar la legibilidad de los datos que obtendremos de las API, definiremos una función que mostrará cadenas JSON por pantalla formateadas para mejorar la lectura. La función aceptará tanto cadenas de carácteres con contenido JSON como objetos Python, y mostrará el contenido por pantalla.\n",
        "\n",
        "Además, la función recibirá un parámetro opcional que nos permitirá indicar el número máximo de líneas que hay que mostrar. Así, podremos usar la función para visualizar las primeras líneas de un JSON largo, sin tener que mostrar el JSON completo por pantalla."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AB_y8HGcoTla",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define la función 'json_print', que tiene un parámetro obligatorio 'json_data' y un parámetro opcional limit\n",
        "# y no devuelve ningún valor.\n",
        "# La función muestra por pantalla el contenido de la variable 'json_data' en formato JSON, limitando el número \n",
        "# de líneas a mostrar si se incluye el parámetro limit.\n",
        "def json_print(json_data, limit=None):\n",
        "    if isinstance(json_data, (str)):\n",
        "        json_data = json.loads(json_data)\n",
        "    nice = json.dumps(json_data, sort_keys=True, indent=3, separators=(',', ': '))\n",
        "    print(\"\\n\".join(nice.split(\"\\n\")[0:limit]))\n",
        "    if limit is not None:\n",
        "        print(\"[...]\")\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIh1asmBoTlc",
        "colab_type": "text"
      },
      "source": [
        "Veamos un ejemplo del resultado de utilizar la función que acabamos de definir."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3P2Dp0toTlc",
        "colab_type": "code",
        "outputId": "83454c8f-8e7a-4c4a-f516-7431d9765223",
        "colab": {}
      },
      "source": [
        "# Muestra el valor de la variable 'json_ejemplo' con la función 'print'.\n",
        "json_ejemplo = '{\"nombre\": \"Yann\", \"apellidos\": {\"apellido1\": \"LeCun\", \"apellido2\": \"-\"}, \"edad\": 56}'\n",
        "print(json_ejemplo)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"nombre\": \"Yann\", \"apellidos\": {\"apellido1\": \"LeCun\", \"apellido2\": \"-\"}, \"edad\": 56}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39ObnABgoTlf",
        "colab_type": "code",
        "outputId": "dc466235-7509-408e-b2ef-c17eddb53a30",
        "colab": {}
      },
      "source": [
        "# Muestra el valor de la variable 'json_ejemplo' con la función 'json_print' que acabamos de definir.\n",
        "json_print(json_ejemplo)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "   \"apellidos\": {\n",
            "      \"apellido1\": \"LeCun\",\n",
            "      \"apellido2\": \"-\"\n",
            "   },\n",
            "   \"edad\": 56,\n",
            "   \"nombre\": \"Yann\"\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10Msrg8WoTli",
        "colab_type": "code",
        "outputId": "c65c8424-0b30-4a4f-9322-692f2c9ebd54",
        "colab": {}
      },
      "source": [
        "# Mostramos únicamente las tres primeras líneas.\n",
        "json_print(json_ejemplo, 3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "   \"apellidos\": {\n",
            "      \"apellido1\": \"LeCun\",\n",
            "[...]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ijA6ctkoTlk",
        "colab_type": "text"
      },
      "source": [
        "## Descarga directa de datos\n",
        "\n",
        "La descarga directa del conjunto de datos es quizás el método más sencillo de adquisición de datos y consiste en descargar un fichero con los datos de interés ya recopilados por algún otro analista. De hecho, en la unidad anterior ya hemos usado este método para adquirir el fichero con los datos sobre los personajes de cómic de Marvel. Una vez descargado el fichero, el procedimiento para cargarlo en Python dependerá del formato concreto (ya hemos visto un ejemplo de carga de datos desde un fichero .csv).\n",
        "\n",
        "Algunos de los sitios web donde podéis encontrar conjuntos de datos para analizar son:\n",
        "- [Open Data gencat](http://dadesobertes.gencat.cat/en/), el portal de datos abiertos de la Generalitat.\n",
        "- [datos.gov.es](http://datos.gob.es/es/catalogo), el catálogo de conjuntos de datos del Gobierno de España.\n",
        "- [European Data Sources](https://data.europa.eu/), el portal de datos abiertos de la Unión Europea.\n",
        "- [Mark Newman network datasets](http://www-personal.umich.edu/~mejn/netdata/), conjuntos de datos en forma de red recopilados por Mark Newman.\n",
        "- [Stanford Large Network Dataset Collection](http://snap.stanford.edu/data/), otra recopilación de conjuntos de datos en forma de red, en este caso creado por Jure Leskovec.\n",
        "- [SecRepo.com](http://www.secrepo.com/), datos relacionados con la seguridad.\n",
        "- [AWS Public Datasets](https://aws.amazon.com/public-datasets/), conjuntos de datos recopilados y hospedados por Amazon.\n",
        "- [UC Irvine Machine Learning Repository](http://archive.ics.uci.edu/ml/), datos recopilados por un grupo de investigación de la Universidad de California en Irvine.\n",
        "- El [repositorio de Five Thirty Eight](https://github.com/fivethirtyeight), que recoge datos utilizados en artículos de la publicación y que ya hemos visto en la unidad anterior.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_gKsiZUoTll",
        "colab_type": "text"
      },
      "source": [
        "## Uso de API de terceros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4rsD7eToTll",
        "colab_type": "text"
      },
      "source": [
        "### Acceso a API manualmente\n",
        "\n",
        "Podemos utilizar la librería de Python [Requests](http://docs.python-requests.org/) para realizar peticiones a web API de manera manual. Para ello, tendremos que acceder a la documentación de la API con la que queramos actuar, construir manualmente las peticiones para obtener la información deseada y procesar también manualmente la respuesta recibida.\n",
        "\n",
        "Veamos un ejemplo de petición HTTP a una API pública. El sitio http://postcodes.io/ ofrece una API de geolocalización sobre códigos postales en el Reino Unido. Leyendo la documentación, podemos ver que tiene un método GET con la URL http://api.postcodes.io/postcodes/:código-postal que nos retorna información del código postal especificado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9ZRVky4oTlm",
        "colab_type": "code",
        "outputId": "2835c718-813c-4182-8a88-01eecabdeffa",
        "colab": {}
      },
      "source": [
        "# Importamos la librería.\n",
        "import requests\n",
        "\n",
        "# Realizamos una petición get a la API, preguntando sobre el código postal \"E98 1TT\"\n",
        "# Notad que el carácter espacio se codifica como %20 en la URL.\n",
        "response = requests.get('http://api.postcodes.io/postcodes/E98%201TT')\n",
        "\n",
        "# Mostramos la respuesta recibida.\n",
        "print(\"Código de estado de la respuesta: \", response.status_code, \"\\n\")\n",
        "print(\"Cabecera de la respuesta: \")\n",
        "json_print(dict(response.headers))\n",
        "print(\"\\nCuerpo de la respuesta: \")\n",
        "json_print(str(response.text))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Código de estado de la respuesta:  200 \n",
            "\n",
            "Cabecera de la respuesta: \n",
            "{\n",
            "   \"Access-Control-Allow-Origin\": \"*\",\n",
            "   \"Connection\": \"keep-alive\",\n",
            "   \"Content-Length\": \"805\",\n",
            "   \"Content-Type\": \"application/json; charset=utf-8\",\n",
            "   \"Date\": \"Mon, 05 Aug 2019 07:45:30 GMT\",\n",
            "   \"ETag\": \"W/\\\"325-s87vTpVPeXA7mcWcF8hfLWqOGL8\\\"\",\n",
            "   \"Server\": \"nginx/1.14.0\",\n",
            "   \"X-GNU\": \"Michael J Blanchard\"\n",
            "}\n",
            "\n",
            "Cuerpo de la respuesta: \n",
            "{\n",
            "   \"result\": {\n",
            "      \"admin_county\": null,\n",
            "      \"admin_district\": \"Tower Hamlets\",\n",
            "      \"admin_ward\": \"St Katharine's & Wapping\",\n",
            "      \"ccg\": \"NHS Tower Hamlets\",\n",
            "      \"ced\": null,\n",
            "      \"codes\": {\n",
            "         \"admin_county\": \"E99999999\",\n",
            "         \"admin_district\": \"E09000030\",\n",
            "         \"admin_ward\": \"E05009330\",\n",
            "         \"ccg\": \"E38000186\",\n",
            "         \"ced\": \"E99999999\",\n",
            "         \"nuts\": \"UKI42\",\n",
            "         \"parish\": \"E43000220\",\n",
            "         \"parliamentary_constituency\": \"E14000882\"\n",
            "      },\n",
            "      \"country\": \"England\",\n",
            "      \"eastings\": 534427,\n",
            "      \"european_electoral_region\": \"London\",\n",
            "      \"incode\": \"1TT\",\n",
            "      \"latitude\": 51.508024,\n",
            "      \"longitude\": -0.064393,\n",
            "      \"lsoa\": \"Tower Hamlets 026B\",\n",
            "      \"msoa\": \"Tower Hamlets 026\",\n",
            "      \"nhs_ha\": \"London\",\n",
            "      \"northings\": 180564,\n",
            "      \"nuts\": \"Tower Hamlets\",\n",
            "      \"outcode\": \"E98\",\n",
            "      \"parish\": \"Tower Hamlets, unparished area\",\n",
            "      \"parliamentary_constituency\": \"Poplar and Limehouse\",\n",
            "      \"postcode\": \"E98 1TT\",\n",
            "      \"primary_care_trust\": \"Tower Hamlets\",\n",
            "      \"quality\": 1,\n",
            "      \"region\": \"London\"\n",
            "   },\n",
            "   \"status\": 200\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trfYwqi4oTlo",
        "colab_type": "text"
      },
      "source": [
        "Como podemos ver, el estado de la respuesta es 200, lo que [nos indica](https://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html) que la petición se ha procesado correctamente. Entre otros campos, la cabecera de la respuesta incluye el tipo de contenido que encontraremos en el cuerpo, que será un texto en formato JSON. Por último, el cuerpo de la respuesta incluye datos sobre el código postal consultado. Por ejemplo, podemos ver que corresponde a Inglaterra (concretamente, a la ciudad de Londres).\n",
        "\n",
        "Notad que podemos visualizar también la respuesta accediendo a la [misma URL](http://api.postcodes.io/postcodes/E98%201TT) con un navegador web. En este caso, se pueden instalar extensiones específicas que gestionen la visualización mejorada del JSON retornado (por ejemplo, [JSONView](https://chrome.google.com/webstore/detail/jsonview/chklaanhfefbnpoihckbnefhakgolnmc) para Chrome o Firefox)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRQJ7f4DoTlp",
        "colab_type": "text"
      },
      "source": [
        "### Acceso a API con librerías de Python\n",
        "\n",
        "Aunque podríamos usar este método para interactuar con cualquier API HTTP, lo cierto es que cuando la complejidad de las funciones disponibles incrementa (por ejemplo, al incluir autenticación) puede no resultar muy práctico. Cuando queramos acceder a APIs populares, normalmente encontraremos que ya existen librerías de Python diseñadas para interactuar con las estas APIs, de manera que podremos obtener datos sin necesidad de manejar las peticiones HTTP manualmente.\n",
        "\n",
        "Por ejemplo, Twitter, la famosa plataforma de envío de mensajes cortos, ofrece varias [APIs](https://developer.twitter.com/en/docs/api-reference-index) que permiten obtener datos de la red. Disponemos de varias librerías de Python que permiten interactuar con la API de Twitter. En este notebook, veremos cómo obtener datos de Twitter usando [Tweepy](http://www.tweepy.org/).\n",
        "\n",
        "#### Autenticación con la API de Twitter\n",
        "\n",
        "Twitter requiere autenticación para poder utilizar su API. Por este motivo, el primer paso a realizar para poder obtener datos de Twitter a través de su API es conseguir unas credenciales adecuadas. En esta sección, describiremos cómo obtener credenciales para acceder a la API de Twitter.\n",
        "\n",
        "Para empezar, es necesario disponer de una cuenta en Twitter. Para poder ejecutar los ejemplos del notebook, necesitaréis por lo tanto tener una cuenta de Twitter. Podéis utilizar vuestra cuenta personal, si ya disponéis de ella, para solicitar los permisos de desarrollador que nos permitirán interactuar con la API. En caso contrario (o si preferís no usar vuestra cuenta personal), podéis crearos una cuenta de Twitter nueva. El proceso es muy sencillo:\n",
        "1. Acceder a [Twitter](http://www.twitter.com).\n",
        "2. Pulsar sobre *Sign up for Twitter* y seguir las indicaciones para completar el registro.\n",
        "\n",
        "Después, habrá que solicitar convertir la cuenta recién creada (o vuestra cuenta personal), en una cuenta de desarrollador. Para hacerlo, hay que seguir los siguientes pasos:\n",
        "1. Acceder al [panel de desarolladores de Twitter](https://developer.twitter.com/).\n",
        "2. Clickar sobre *Apply*.\n",
        "3. Clickar sobre *Apply for a developer account*.\n",
        "3. Pulsar *Continue*.\n",
        "4. Indicar porqué queréis disponer de una cuenta de desarrollador.\n",
        "\n",
        "Para poder realizar este proceso satisfactoriamente, necesitaréis que vuestra cuenta disponga de un número de teléfono asociado verificado. En caso contrario, veréis que os aparecerá un mensaje para que verifiquéis vuestro teléfono. \n",
        "\n",
        "Finalmente, una vez ya disponemos de una cuenta en Twitter, será necesario registrar una nueva aplicación. Para hacerlo, es necesario seguir los siguientes pasos:\n",
        "1. Acceder al [panel de desarolladores de Twitter](https://developer.twitter.com/en/apps).\n",
        "2. Pulsar sobre *Create new app*.\n",
        "3. Rellenar el formulario con los detalles de la aplicación. En concreto, necesitaréis proporcionar como mínimo los campos:\n",
        "    * *App name*\n",
        "    * *Application description*\n",
        "    * *Website URL*\n",
        "    * *Tell us how this app will be used*\n",
        "\n",
        "El campo Website debe contener una URL válida (por ejemplo, el enlace a vuestro perfil de Twitter).\n",
        "\n",
        "Una vez creada la aplicación, podéis acceder a la pestaña *Keys and access tokens*. Allí se encuentran las credenciales recién creadas para vuestra aplicación, que utilizaremos para autenticarnos y poder utilizar la API de Twitter. Veréis que ya tenéis las claves *Consumer API keys* disponibles. Además, será necesario pulsar sobre *Create* en la sección *Access token & access token secret* para obtener también ambos tokens. Los cuatro valores serán usados para autenticar nuestra aplicación:\n",
        "* API / Consumer Key\n",
        "* API / Consumer Secret\n",
        "* Access Token\n",
        "* Access Token Secret\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vzxtk0YoTlp",
        "colab_type": "text"
      },
      "source": [
        "#### La librería Tweepy\n",
        "\n",
        "[Tweepy](http://www.tweepy.org/) nos permite interactuar con la API de Twitter de una manera sencilla, ya que encapsula los métodos HTTP de la API en métodos de Python, que pueden ser llamados directamente. Encontraréis la documentación de la librería en el siguiente [enlace](http://tweepy.readthedocs.io)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtjxV9m1oTlq",
        "colab_type": "code",
        "outputId": "cafef47d-c381-4e55-8e22-7efdc33c8a9c",
        "colab": {}
      },
      "source": [
        "# Importamos la librería tweepy\n",
        "import tweepy\n",
        "\n",
        "# IMPORTANTE: Es necesario incluir las credenciales de acceso que hayáis obtenido al crear vuestra App\n",
        "# para ejecutar el ejemplo.\n",
        "consumer_key = ''\n",
        "consumer_secret = ''\n",
        "access_token = ''\n",
        "access_secret = ''\n",
        "\n",
        "# Inicializamos la interacción con la API\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_secret)\n",
        "api = tweepy.API(auth)\n",
        "\n",
        "# Obtenemos datos del usuario \"twitter\" usando la librería tweepy\n",
        "user = api.get_user('twitter')\n",
        "\n",
        "print(\"El tipo de datos de la variable user es: {}\".format(type(user)))\n",
        "print(\"El nombre de usuario es: {}\".format(user.screen_name))\n",
        "print(\"El id de usuario es: {}\".format(user.id))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "El tipo de datos de la variable user es: <class 'tweepy.models.User'>\n",
            "El nombre de usuario es: Twitter\n",
            "El id de usuario es: 783214\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbLuQtVsoTlt",
        "colab_type": "text"
      },
      "source": [
        "Notad que, en este caso, no hemos tenido que gestionar las peticiones HTTP manualmente: la librería lo ha hecho por nosotros de forma transparente. \n",
        "\n",
        "Además, las funciones de la librería nos devuelven directamente objetos Python, que pueden ser usados como cualquier otro. Por ejemplo, podemos seleccionar solo una parte de las respuestas de las APIs según nuestro interés (en el ejemplo anterior, hemos seleccionado el identificador y el nombre de usuario directamente usando el objeto `user`). Veamos algunos ejemplos más de atributos que hemos recuperado del usuario:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zppEahdgoTlt",
        "colab_type": "code",
        "outputId": "472c9884-3e0b-4427-8d4f-21dbc51da88f",
        "colab": {}
      },
      "source": [
        "# Mostramos algunos atributos del usuario recuperado\n",
        "print(\"El número de seguidores es: {}\".format(user.followers_count))\n",
        "print(\"El número de amigos es: {}\".format(user.friends_count))\n",
        "print(\"El número de tweets es: {}\".format(user.statuses_count))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "El número de seguidores es: 56440698\n",
            "El número de amigos es: 29\n",
            "El número de tweets es: 11114\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe1CdgnIoTlv",
        "colab_type": "text"
      },
      "source": [
        "## Capturando datos manualmente: _web crawling_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdY_2JBUoTlx",
        "colab_type": "text"
      },
      "source": [
        "[Scrapy](https://scrapy.org/) es una librería de Python que provee de un _framework_ para la extracción de datos de páginas web. Scrapy es muy completa y dispone de múltiples funcionalidades, pero veremos un ejemplo sencillo de su uso.\n",
        "\n",
        "Suponed que queremos obtener un listado de las titulaciones de grado que ofrece la UOC. La UOC no ofrece una API con esta información, pero sí que podemos encontrarla en la página http://estudios.uoc.edu/es/grados. De todos modos, no queremos ir copiando manualmente los nombres de todas las titulaciones para obtener el listado de interés, por lo que desarollaremos un pequeño *crawler* que obtenga estos datos por nosotros.\n",
        "\n",
        "Ya tenemos identificada la url que queremos explorar (http://estudios.uoc.edu/es/grados), así que solo será necesario identificar dónde se encuentran los datos de interés dentro de la página. Para hacerlo, en primer lugar nos fijaremos en algún título de grado que aparezca en la página, por ejemplo, Diseño y Creación Digitales o Multimedia. Seguidamente accederemos al código fuente de la página (podemos usar la combinación de teclas `CTRL + u` en los navegadores Firefox o Chrome) y buscaremos los nombres de los grados que hemos visto anteriormente:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1hfdi_-oTlx",
        "colab_type": "raw"
      },
      "source": [
        "<a \n",
        "   title=\"Diseño y Creación Digitales\" \n",
        "   href=\"/es/grados/diseño-creacion-digital/presentacion\" \n",
        "   class=\"card-absolute-link\">\n",
        "    &nbsp;\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICECIKacoTlx",
        "colab_type": "raw"
      },
      "source": [
        "<a \n",
        "   title=\"Multimedia\" \n",
        "   href=\"/es/grados/multimedia/presentacion\" \n",
        "   class=\"card-absolute-link\">\n",
        "    &nbsp;\n",
        "</a> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xr4NvhHvoTly",
        "colab_type": "text"
      },
      "source": [
        "Como se puede apreciar, los datos que queremos recopilar (los nombres de las titulaciones de grado que ofrece la UOC) se encuentran en el atributo título (_title_) de un hipervínculo (un elemento señalado con la etiqueta `<a>`) que tiene el atributo clase fijado a «card-absolute-link». \n",
        "\n",
        "Para indicar que queremos seleccionar estos datos, utilizaremos la sintaxis XPath. En concreto, utilizaremos la expresión:\n",
        "```\n",
        "//a[@class=\"card-absolute-link\"]/@title\n",
        "```\n",
        "\n",
        "que nos indica que queremos seleccionar todas las etiquetas `<a>` que tengan como atributo clase el valor \"card-absolute-link\" y de ellas extraer el título. Con esto ya podemos programar nuestra araña para que extraiga los datos de interés.\n",
        "\n",
        "La estructura de un *crawler* con Scrapy viene prefijada. En nuestro caso, solo será necesario definir una araña e incluir un *parser* que extraiga los datos de las titulaciones y que disponga de la URL de inicio."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7N1i_U4CIC9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a039336f-5702-4fe6-a45d-320594b0467c"
      },
      "source": [
        "pip install scrapy"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scrapy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e0/b7/06c19d7d7f5318ffd1d31d7cd7d944ed9dcf773981c731285350961d9b5c/Scrapy-2.0.1-py2.py3-none-any.whl (242kB)\n",
            "\r\u001b[K     |█▍                              | 10kB 16.6MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |████                            | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 51kB 1.8MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 71kB 2.4MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 81kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 92kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 102kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 112kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 122kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 133kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 143kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 153kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 163kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 174kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 184kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 194kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 204kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 215kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 225kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 235kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 245kB 2.7MB/s \n",
            "\u001b[?25hCollecting w3lib>=1.17.0\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/45/1ba17c50a0bb16bd950c9c2b92ec60d40c8ebda9f3371ae4230c437120b6/w3lib-1.21.0-py2.py3-none-any.whl\n",
            "Collecting protego>=0.1.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/db/6e/bf6d5e4d7cf233b785719aaec2c38f027b9c2ed980a0015ec1a1cced4893/Protego-0.1.16.tar.gz (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 8.8MB/s \n",
            "\u001b[?25hCollecting cssselect>=0.9.1\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Collecting parsel>=1.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/86/c8/fc5a2f9376066905dfcca334da2a25842aedfda142c0424722e7c497798b/parsel-1.5.2-py2.py3-none-any.whl\n",
            "Collecting Twisted>=17.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b7/04/1a664c9e5ec0224a1c1a154ddecaa4dc7b8967521bba225efcc41a03d5f3/Twisted-20.3.0-cp36-cp36m-manylinux1_x86_64.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 32.7MB/s \n",
            "\u001b[?25hCollecting PyDispatcher>=2.0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/cd/37/39aca520918ce1935bea9c356bcbb7ed7e52ad4e31bff9b943dfc8e7115b/PyDispatcher-2.0.5.tar.gz\n",
            "Collecting cryptography>=2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/9a/7cece52c46546e214e10811b36b2da52ce1ea7fa203203a629b8dfadad53/cryptography-2.8-cp34-abi3-manylinux2010_x86_64.whl (2.3MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 44.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.5.0 in /usr/local/lib/python3.6/dist-packages (from scrapy) (4.2.6)\n",
            "Collecting service-identity>=16.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/7c/2195b890023e098f9618d43ebc337d83c8b38d414326685339eb024db2f6/service_identity-18.1.0-py2.py3-none-any.whl\n",
            "Collecting zope.interface>=4.1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/16/38b97756deceb091800de76d5075b898b2b167f4b1f14d041f099daeab35/zope.interface-5.0.2-cp36-cp36m-manylinux2010_x86_64.whl (226kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 48.0MB/s \n",
            "\u001b[?25hCollecting queuelib>=1.4.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4c/85/ae64e9145f39dd6d14f8af3fa809a270ef3729f3b90b3c0cf5aa242ab0d4/queuelib-1.5.0-py2.py3-none-any.whl\n",
            "Collecting pyOpenSSL>=16.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/de/f8342b68fa9e981d348039954657bdf681b2ab93de27443be51865ffa310/pyOpenSSL-19.1.0-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from w3lib>=1.17.0->scrapy) (1.12.0)\n",
            "Collecting Automat>=0.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/dd/83/5f6f3c1a562674d65efc320257bdc0873ec53147835aeef7762fe7585273/Automat-20.2.0-py2.py3-none-any.whl\n",
            "Collecting PyHamcrest!=1.10.0,>=1.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/16/e54cc65891f01cb62893540f44ffd3e8dab0a22443e1b438f1a9f5574bee/PyHamcrest-2.0.2-py3-none-any.whl (52kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.6MB/s \n",
            "\u001b[?25hCollecting constantly>=15.1\n",
            "  Downloading https://files.pythonhosted.org/packages/b9/65/48c1909d0c0aeae6c10213340ce682db01b48ea900a7d9fce7a7910ff318/constantly-15.1.0-py2.py3-none-any.whl\n",
            "Collecting incremental>=16.10.1\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/1d/c98a587dc06e107115cf4a58b49de20b19222c83d75335a192052af4c4b7/incremental-17.5.0-py2.py3-none-any.whl\n",
            "Collecting hyperlink>=17.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/7f/91/e916ca10a2de1cb7101a9b24da546fb90ee14629e23160086cf3361c4fb8/hyperlink-19.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.6/dist-packages (from Twisted>=17.9.0->scrapy) (19.3.0)\n",
            "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.0->scrapy) (1.14.0)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.6/dist-packages (from service-identity>=16.0.0->scrapy) (0.2.8)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.6/dist-packages (from service-identity>=16.0.0->scrapy) (0.4.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from zope.interface>=4.1.3->scrapy) (46.0.0)\n",
            "Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.6/dist-packages (from hyperlink>=17.1.1->Twisted>=17.9.0->scrapy) (2.8)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.0->scrapy) (2.20)\n",
            "Building wheels for collected packages: protego, PyDispatcher\n",
            "  Building wheel for protego (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for protego: filename=Protego-0.1.16-cp36-none-any.whl size=7765 sha256=74b96a4e5cd5cb72e3bd1c441ae7307e48bde8679580526cbf0855bd95bc6b6d\n",
            "  Stored in directory: /root/.cache/pip/wheels/51/01/d1/4a2286a976dccd025ba679acacfe37320540df0f2283ecab12\n",
            "  Building wheel for PyDispatcher (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.5-cp36-none-any.whl size=11515 sha256=d0e3d1616797730031ed71e7b79abf042c7cb22e1686b76ec641dc0afe1048f5\n",
            "  Stored in directory: /root/.cache/pip/wheels/88/99/96/cfef6665f9cb1522ee6757ae5955feedf2fe25f1737f91fa7f\n",
            "Successfully built protego PyDispatcher\n",
            "Installing collected packages: w3lib, protego, cssselect, parsel, Automat, PyHamcrest, constantly, incremental, hyperlink, zope.interface, Twisted, PyDispatcher, cryptography, service-identity, queuelib, pyOpenSSL, scrapy\n",
            "Successfully installed Automat-20.2.0 PyDispatcher-2.0.5 PyHamcrest-2.0.2 Twisted-20.3.0 constantly-15.1.0 cryptography-2.8 cssselect-1.1.0 hyperlink-19.0.0 incremental-17.5.0 parsel-1.5.2 protego-0.1.16 pyOpenSSL-19.1.0 queuelib-1.5.0 scrapy-2.0.1 service-identity-18.1.0 w3lib-1.21.0 zope.interface-5.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuhajnSuoTly",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importamos scrapy.\n",
        "import scrapy\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "\n",
        "# Creamos la araña.\n",
        "class uoc_spider(scrapy.Spider):\n",
        "    \n",
        "    # Asignamos un nombre a la araña.\n",
        "    name = \"uoc_spider\"\n",
        "    \n",
        "    # Indicamos la url que queremos analizar en primer lugar.\n",
        "    start_urls = [\n",
        "        \"http://estudios.uoc.edu/es/grados\"\n",
        "    ]\n",
        "\n",
        "    # Definimos el analizador.\n",
        "    def parse(self, response):\n",
        "        # Extraemos el título del grado.\n",
        "        for grado in response.xpath('//a[@class=\"card-absolute-link\"]/@title'):\n",
        "            yield {\n",
        "                'title': grado.extract()\n",
        "            }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Qvamc_voTl1",
        "colab_type": "text"
      },
      "source": [
        "Una vez definida la araña, lanzaremos el *crawler* indicando que queremos que use la araña `uoc_spider` que acabamos de definir:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSbqeLqKoTl1",
        "colab_type": "code",
        "outputId": "1d558eef-bff3-42e8-fac5-b843adedf1a2",
        "colab": {}
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Creamos un crawler.\n",
        "    process = CrawlerProcess({\n",
        "        'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n",
        "        'DOWNLOAD_HANDLERS': {'s3': None},\n",
        "        'LOG_ENABLED': True\n",
        "    })\n",
        "\n",
        "    # Inicializamos el crawler con nuestra araña.\n",
        "    process.crawl(uoc_spider)\n",
        "    \n",
        "    # Lanzamos la araña.\n",
        "    process.start()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-08-05 10:10:42 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: scrapybot)\n",
            "2019-08-05 10:10:42 [scrapy.utils.log] INFO: Versions: lxml 4.4.0.0, libxml2 2.9.9, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 19.2.1, Python 3.6.8 (default, Jan 14 2019, 11:02:34) - [GCC 8.0.1 20180414 (experimental) [trunk revision 259383]], pyOpenSSL 19.0.0 (OpenSSL 1.1.1  11 Sep 2018), cryptography 2.1.4, Platform Linux-4.15.0-55-generic-x86_64-with-Ubuntu-18.04-bionic\n",
            "2019-08-05 10:10:42 [scrapy.crawler] INFO: Overridden settings: {'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
            "2019-08-05 10:10:42 [scrapy.extensions.telnet] INFO: Telnet Password: 6a3bf1009206bc6b\n",
            "2019-08-05 10:10:42 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2019-08-05 10:10:42 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2019-08-05 10:10:42 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2019-08-05 10:10:42 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2019-08-05 10:10:42 [scrapy.core.engine] INFO: Spider opened\n",
            "2019-08-05 10:10:42 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2019-08-05 10:10:42 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2019-08-05 10:10:42 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://estudios.uoc.edu/es/grados> from <GET http://estudios.uoc.edu/es/grados>\n",
            "2019-08-05 10:10:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://estudios.uoc.edu/es/grados> (referer: None)\n",
            "2019-08-05 10:10:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://estudios.uoc.edu/es/grados>\n",
            "{'title': 'Antropología y Evolución Humana (interuniversitario: URV, UOC)'}\n",
            "2019-08-05 10:10:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://estudios.uoc.edu/es/grados>\n",
            "{'title': 'Artes'}\n",
            "2019-08-05 10:10:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://estudios.uoc.edu/es/grados>\n",
            "{'title': 'Ciencias Sociales'}\n",
            "2019-08-05 10:10:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://estudios.uoc.edu/es/grados>\n",
            "{'title': 'Historia, Geografia e Historia del Arte (interuniversitario: UOC, UdL)'}\n",
            "2019-08-05 10:10:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://estudios.uoc.edu/es/grados>\n",
            "{'title': 'Humanidades'}\n",
            "2019-08-05 10:10:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://estudios.uoc.edu/es/grados>\n",
            "{'title': 'Lengua y Literatura Catalanas'}\n",
            "2019-08-05 10:10:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://estudios.uoc.edu/es/grados>\n",
            "{'title': 'Traducción, Interpretación y Lenguas Aplicadas (interuniversitario: UVic-UCC, UOC)'}\n",
            "2019-08-05 10:10:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://estudios.uoc.edu/es/grados>\n",
            "{'title': 'Logopedia (interuniversitario: Uvic-UCC, UOC)'}\n",
            "2019-08-05 10:10:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://estudios.uoc.edu/es/grados>\n",
            "{'title': 'Comunicación'}\n",
            "2019-08-05 10:10:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://estudios.uoc.edu/es/grados>\n",
            "{'title': 'Diseño y Creación Digitales'}\n",
            "2019-08-05 10:10:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://estudios.uoc.edu/es/grados>\n",
            "{'title': 'Criminología'}\n",
            "2019-08-05 10:10:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://estudios.uoc.edu/es/grados>\n",
            "{'title': 'Derecho'}\n",
            "2019-08-05 10:10:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://estudios.uoc.edu/es/grados>\n",
            "{'title': 'Doble titulación de Derecho y de Administración y Dirección de Empresas'}\n",
            "2019-08-05 10:10:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://estudios.uoc.edu/es/grados>\n",
            "{'title': 'Gestión y Administración Pública (interuniversitario: UOC, UB)'}\n",
            "2019-08-05 10:10:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://estudios.uoc.edu/es/grados>\n",
            "{'title': 'Relaciones Internacionales'}\n",
            "2019-08-05 10:10:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://estudios.uoc.edu/es/grados>\n",
            "{'title': 'Artes'}\n",
            "2019-08-05 10:10:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://estudios.uoc.edu/es/grados>\n",
            "{'title': 'Diseño y Creación Digitales'}\n",
            "2019-08-05 10:10:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://estudios.uoc.edu/es/grados>\n",
            "{'title': 'Multimedia'}\n",
            "2019-08-05 10:10:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://estudios.uoc.edu/es/grados>\n",
            "{'title': 'Administración y Dirección de Empresas'}\n",
            "2019-08-05 10:10:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://estudios.uoc.edu/es/grados>\n",
            "{'title': 'Doble titulación de Administración y Dirección de Empresas y de Turismo'}\n",
            "2019-08-05 10:10:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://estudios.uoc.edu/es/grados>\n",
            "{'title': 'Economía'}\n",
            "2019-08-05 10:10:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://estudios.uoc.edu/es/grados>\n",
            "{'title': 'Marketing e investigación de mercados'}\n",
            "2019-08-05 10:10:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://estudios.uoc.edu/es/grados>\n",
            "{'title': 'Relaciones Laborales y Ocupación'}\n",
            "2019-08-05 10:10:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://estudios.uoc.edu/es/grados>\n",
            "{'title': 'Ciencia de Datos Aplicada /Applied Data Science'}\n",
            "2019-08-05 10:10:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://estudios.uoc.edu/es/grados>\n",
            "{'title': 'Doble titulación de Ingeniería Informática y de Administración y Dirección de Empresas'}\n",
            "2019-08-05 10:10:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://estudios.uoc.edu/es/grados>\n",
            "{'title': 'Ingeniería Informática'}\n",
            "2019-08-05 10:10:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://estudios.uoc.edu/es/grados>\n",
            "{'title': 'Ingeniería de Tecnologías y Servicios de Telecomunicación'}\n",
            "2019-08-05 10:10:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://estudios.uoc.edu/es/grados>\n",
            "{'title': 'Multimedia'}\n",
            "2019-08-05 10:10:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://estudios.uoc.edu/es/grados>\n",
            "{'title': 'Educación Social'}\n",
            "2019-08-05 10:10:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://estudios.uoc.edu/es/grados>\n",
            "{'title': 'Psicología'}\n",
            "2019-08-05 10:10:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://estudios.uoc.edu/es/grados>\n",
            "{'title': 'Turismo'}\n",
            "2019-08-05 10:10:44 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2019-08-05 10:10:44 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 480,\n",
            " 'downloader/request_count': 2,\n",
            " 'downloader/request_method_count/GET': 2,\n",
            " 'downloader/response_bytes': 453717,\n",
            " 'downloader/response_count': 2,\n",
            " 'downloader/response_status_count/200': 1,\n",
            " 'downloader/response_status_count/301': 1,\n",
            " 'elapsed_time_seconds': 2.255246,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2019, 8, 5, 8, 10, 44, 795869),\n",
            " 'item_scraped_count': 31,\n",
            " 'log_count/DEBUG': 33,\n",
            " 'log_count/INFO': 10,\n",
            " 'memusage/max': 83087360,\n",
            " 'memusage/startup': 83087360,\n",
            " 'response_received_count': 1,\n",
            " 'scheduler/dequeued': 2,\n",
            " 'scheduler/dequeued/memory': 2,\n",
            " 'scheduler/enqueued': 2,\n",
            " 'scheduler/enqueued/memory': 2,\n",
            " 'start_time': datetime.datetime(2019, 8, 5, 8, 10, 42, 540623)}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-08-05 10:10:44 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdXO7mYDoTl4",
        "colab_type": "text"
      },
      "source": [
        "La ejecución de Scrapy muestra un log detallado con todos los eventos que han ido ocurriendo, lo que es muy útil para identificar problemas, sobre todo en capturas complejas. En nuestro caso, además, podemos ver como se han extraído los nombres de las titulaciones de grado:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6c0S0_voTl4",
        "colab_type": "raw"
      },
      "source": [
        "DEBUG:scrapy.core.scraper:Scraped from <200 http://estudios.uoc.edu/es/grados>\n",
        "{'title': u'Antropolog\\xeda y Evoluci\\xf3n Humana (interuniversitario: URV, UOC)'}\n",
        "DEBUG:scrapy.core.scraper:Scraped from <200 http://estudios.uoc.edu/es/grados>\n",
        "{'title': u'Ciencias Sociales'}\n",
        "DEBUG:scrapy.core.scraper:Scraped from <200 http://estudios.uoc.edu/es/grados>\n",
        "{'title': u'Historia, Geograf\\xeda e Historia del Arte (interuniversitario: UOC, UdL)'}\n",
        "DEBUG:scrapy.core.scraper:Scraped from <200 http://estudios.uoc.edu/es/grados>\n",
        "{'title': u'Humanidades'}\n",
        "DEBUG:scrapy.core.scraper:Scraped from <200 http://estudios.uoc.edu/es/grados>\n",
        "{'title': u'Lengua y Literatura Catalanas'}\n",
        "DEBUG:scrapy.core.scraper:Scraped from <200 http://estudios.uoc.edu/es/grados>\n",
        "{'title': u'Traducci\\xf3n, Interpretaci\\xf3n y Lenguas Aplicadas (interuniversitario: UVic-UCC, UOC)'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNHAe-E-oTl5",
        "colab_type": "text"
      },
      "source": [
        "### Anexo: La API de googlemaps\n",
        "\n",
        "Este anexo contiene un ejemplo adicional de acceso a API con librerías de Python. En concreto, el ejemplo muestra como acceder a la API de googlemaps. En el pasado, el uso de esta API era gratuito, pero actualmente el uso de la API tiene múltiples restricciones y, aunque se pueden realizar algunas peticiones gratuitamente, es necesario proporcionar datos de nuestra targeta de crédito para poder interactuar con la API. Podéis revisar el código de este ejemplo para tener una muestra más del uso de librerías para acceder a APIs, o bien crear una cuenta en la plataforma de google developers y probar los ejemplos proporcionados. En este último caso, recordad revisar la política de cobro de googlemaps, para asegurar que no sobrepasáis el límite gratuito, antes de realizar las pruebas.\n",
        "\n",
        "Google maps dispone de un [conjunto de API](https://developers.google.com/maps/) muy populares que permiten, entre otros, obtener las coordenadas geográficas de una dirección, conseguir indicaciones para desplazarse de un punto a otro, o adquirir datos sobre la elevación del terreno en cualquier punto del mundo. La librería [googlemaps](https://googlemaps.github.io/google-maps-services-python/docs/) integra peticiones a la API de Google en código Python.\n",
        "\n",
        "Para usar las APIs de Google Maps, es necesario registrar un usuario y obtener una clave de autenticación, que adjuntaremos a las peticiones que se realicen contra la API. Además, tendremos que especificar qué APIs concretas vamos a usar. \n",
        "\n",
        "Para el siguiente ejemplo, realizaremos estos tres pasos para obtener la clave de autenticación:\n",
        "\n",
        "1. Crearemos un proyecto en la plataforma de Google Developers.\n",
        "2. Activaremos las APIs deseadas.\n",
        "3. Solicitaremos credenciales de acceso.\n",
        "\n",
        "En primer lugar crearemos un nuevo proyecto en el entorno de desarrolladores de google. Nos dirigiremos a:\n",
        "https://console.developers.google.com/apis/library\n",
        "y haremos clic sobre «Project: New project». Asignaremos un nombre cualquiera al proyecto y confirmaremos la creación pulsando «Create».\n",
        "\n",
        "Una vez creado el proyecto, activaremos las APIs que usaremos después. Primero, seleccionaremos la API de geocodificación ([_Google Maps Geocoding API_](https://console.developers.google.com/apis/api/geocoding_backend)), que se encuentra en la categoría _Google Maps APIs_ (es posible que tengáis que pulsar sobre el botón _more_ para ver la lista completa de APIs). Haremos click sobre _Enable_ para activarla.\n",
        "\n",
        "Repetiremos el proceso para la API de direcciones ([_Google Maps Directions API_](https://console.developers.google.com/apis/api/directions_backend)), que se encuentra también en la categoría _Google Maps APIs_.\n",
        "\n",
        "Finalmente, clickaremos sobre el menú «Credentials», indicaremos «Create credentials» y escogeremos «API Key». Nos aparecerá una ventana con una cadena de caracteres que representa nuestra clave. Para que el siguiente ejemplo funcione, **es necesario que asignéis a la variable api_key el valor de vuestra clave**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGjCJLjqoTl5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importamos la librería googlemaps, que interactuará con la API de google maps.\n",
        "import googlemaps\n",
        "\n",
        "# Importamos la librería datetime, que nos ofrece funciones de manejos de fechas.\n",
        "from datetime import datetime\n",
        "\n",
        "####################################################################################\n",
        "# ATENCIÓN! Asignad a la variable 'api_key' la clave que hayáis obtenido de Google.\n",
        "api_key = \"\"\n",
        "####################################################################################\n",
        "\n",
        "# Inicializamos el cliente, indicando la clave de autenticación.\n",
        "gmaps = googlemaps.Client(key=api_key)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DU2CybJXoTl8",
        "colab_type": "text"
      },
      "source": [
        "En primer lugar, utilizaremos la [API de geocodificación](https://developers.google.com/maps/documentation/geocoding/start) para obtener datos de una dirección a través del método [geocode](https://googlemaps.github.io/google-maps-services-python/docs/2.4.6/#googlemaps.Client.geocode) del cliente de google maps que nos ofrece la librería (almacenado en la variable `gmaps`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PGScxNqoTl9",
        "colab_type": "code",
        "outputId": "dea0dcdf-eb71-4be5-c3a9-924214837b6e",
        "colab": {}
      },
      "source": [
        "# Utilizamos la API de geocodificación para obtener datos de una dirección.\n",
        "geocode_result = gmaps.geocode('Rambla del Poblenou, 156, Barcelona')\n",
        "print(\"------ Resultado de geocode ------\")\n",
        "json_print(geocode_result, 20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------ Resultado de geocode ------\n",
            "[\n",
            "   {\n",
            "      \"address_components\": [\n",
            "         {\n",
            "            \"long_name\": \"156\",\n",
            "            \"short_name\": \"156\",\n",
            "            \"types\": [\n",
            "               \"street_number\"\n",
            "            ]\n",
            "         },\n",
            "         {\n",
            "            \"long_name\": \"Rambla del Poblenou\",\n",
            "            \"short_name\": \"Rambla del Poblenou\",\n",
            "            \"types\": [\n",
            "               \"route\"\n",
            "            ]\n",
            "         },\n",
            "         {\n",
            "            \"long_name\": \"Barcelona\",\n",
            "            \"short_name\": \"Barcelona\",\n",
            "[...]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqga1n2zoTl_",
        "colab_type": "text"
      },
      "source": [
        "Otro ejemplo del uso de la [API de geocodificación](https://developers.google.com/maps/documentation/geocoding/start) utiliza el método [reverse_geocode](https://googlemaps.github.io/google-maps-services-python/docs/2.4.6/#googlemaps.Client.reverse_geocode) para obtener información sobre unas coordenadas geográficas concretas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bj0wzKpOoTl_",
        "colab_type": "code",
        "outputId": "3c6a10e8-7d83-4452-ebb7-11120680b2f9",
        "colab": {}
      },
      "source": [
        "# Obtenemos datos sobre unas coordenadas geográficas.\n",
        "reverse_geocode_result = gmaps.reverse_geocode((41.2768089, 1.9884642))\n",
        "print(\"------ Resultado de reverse geocode ------\")\n",
        "json_print(reverse_geocode_result, 20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------ Resultado de reverse geocode ------\n",
            "[\n",
            "   {\n",
            "      \"address_components\": [\n",
            "         {\n",
            "            \"long_name\": \"17\",\n",
            "            \"short_name\": \"17\",\n",
            "            \"types\": [\n",
            "               \"street_number\"\n",
            "            ]\n",
            "         },\n",
            "         {\n",
            "            \"long_name\": \"Avinguda del Canal Ol\\u00edmpic\",\n",
            "            \"short_name\": \"Av. del Canal Ol\\u00edmpic\",\n",
            "            \"types\": [\n",
            "               \"route\"\n",
            "            ]\n",
            "         },\n",
            "         {\n",
            "            \"long_name\": \"Castelldefels\",\n",
            "            \"short_name\": \"Castelldefels\",\n",
            "[...]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sZgVGuloTmC",
        "colab_type": "text"
      },
      "source": [
        "El siguiente ejemplo interactúa con la [API de direcciones](https://developers.google.com/maps/documentation/directions/), usando el método [_directions_](https://googlemaps.github.io/google-maps-services-python/docs/2.4.6/#googlemaps.Client.directions) de la librería googlemaps de Python, para obtener indicaciones de desplazamiento entre dos puntos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOVaIu2goTmC",
        "colab_type": "code",
        "outputId": "cbea1499-070b-41bc-f39e-5c12d3de856a",
        "colab": {}
      },
      "source": [
        "# Obtenemos indicaciones sobre cómo ir de una dirección a otra, considerando el tráfico del momento actual.\n",
        "now = datetime.now()\n",
        "directions_result = gmaps.directions(\"Carrer Colom, 114, Terrassa\",\n",
        "                                     \"Carrer Sant Antoni, 1, Salt\",\n",
        "                                     mode=\"transit\",\n",
        "                                     departure_time=now)\n",
        "print(\"------ Resultado de directions ------\")\n",
        "json_print(directions_result, 15)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------ Resultado de directions ------\n",
            "[\n",
            "   {\n",
            "      \"bounds\": {\n",
            "         \"northeast\": {\n",
            "            \"lat\": 41.98102,\n",
            "            \"lng\": 2.817006\n",
            "         },\n",
            "         \"southwest\": {\n",
            "            \"lat\": 41.481153,\n",
            "            \"lng\": 2.014348\n",
            "         }\n",
            "      },\n",
            "      \"copyrights\": \"Map data \\u00a92017 Google, Inst. Geogr. Nacional\",\n",
            "      \"legs\": [\n",
            "         {\n",
            "[...]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1UNuwlnoTmE",
        "colab_type": "code",
        "outputId": "9c5623e1-62bc-4ee5-db82-8d744d2d7cf5",
        "colab": {}
      },
      "source": [
        "# Mostramos las claves del diccionario que devuelve la llamada a geocode.\n",
        "geocode_result[0].keys()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[u'geometry',\n",
              " u'address_components',\n",
              " u'place_id',\n",
              " u'formatted_address',\n",
              " u'types']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDRECX62oTmG",
        "colab_type": "code",
        "outputId": "7e17ec48-dfb0-4c03-eb2d-b7ce06789241",
        "colab": {}
      },
      "source": [
        "# Mostramos únicamente las coordenadas geográficas de la dirección de interés.\n",
        "geocode_result[0][\"geometry\"][\"location\"]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{u'lat': 41.4063554, u'lng': 2.1947451}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7dQCqbDoTmJ",
        "colab_type": "code",
        "outputId": "e48e59b1-0884-4527-e46c-529c80c9ee2c",
        "colab": {}
      },
      "source": [
        "# Mostramos las localizaciones cercanas a las coordenadas geográficas que hemos preguntado con reverse_geocode, \n",
        "# imprimiendo las coordenadas exactas y la dirección.\n",
        "for result in reverse_geocode_result:\n",
        "    print(result[\"geometry\"][\"location\"], result[\"formatted_address\"])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{u'lat': 41.2772149, u'lng': 1.9892062} Av. del Canal Olímpic, 17, 08860 Castelldefels, Barcelona, Spain\n",
            "{u'lat': 41.2800161, u'lng': 1.9766294} Castelldefels, Barcelona, Spain\n",
            "{u'lat': 41.2790599, u'lng': 1.9734743} Castelldefels, Barcelona, Spain\n",
            "{u'lat': 41.2792267, u'lng': 1.9636914} 08860 Sitges, Barcelona, Spain\n",
            "{u'lat': 41.3847492, u'lng': 1.949021} El Baix Llobregat, Barcelona, Spain\n",
            "{u'lat': 41.383401, u'lng': 2.027319} Barcelona Metropolitan Area, Barcelona, Spain\n",
            "{u'lat': 41.3850477, u'lng': 2.1733131} Barcelona, Spain\n",
            "{u'lat': 41.5911589, u'lng': 1.5208624} Catalonia, Spain\n",
            "{u'lat': 40.46366700000001, u'lng': -3.74922} Spain\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbcYy6m3oTmL",
        "colab_type": "code",
        "outputId": "8470f45c-1e1f-4863-b995-92334e590cdd",
        "colab": {}
      },
      "source": [
        "# Mostramos únicamente la distancia del trayecto entre los dos puntos preguntados a la API de direcciones.\n",
        "print(directions_result[0][\"legs\"][0][\"distance\"])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{u'text': u'112 km', u'value': 112026}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3gYy-NHr8VO",
        "colab_type": "text"
      },
      "source": [
        "### Ejercicio 1\n",
        "Queremos saber los crímenes que se han producido en Reino Unido en una localización (latitud, longitud) y fecha concretas. Identificad qué métodos de la API siguiente podemos utilizar para obtener la información y contestad a las siguientes preguntas.<span style=\"font-family: Courier New; background-color: #82b74b; color: #000000; padding: 3px; \">NM</span> \n",
        "\n",
        "1. ¿A qué URL haremos la petición?\n",
        "2. ¿Qué tipo de petición HTTP (qué acción) deberemos realizar contra la API para obtener los datos deseados?\n",
        "3. ¿En qué formato obtendremos la respuesta de la API?\n",
        "4. ¿Qué parámetros deberemos proporcionar en la petición a la API?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAoJ1sLFIINU",
        "colab_type": "text"
      },
      "source": [
        "Pasos para resolver esto: \n",
        "  \\n 1º - Buscamos en google: data crimes uk api. Esto nos da : https://data.police.uk/docs/ . Vemos la documentación de la API y vemos que proporciona localización, latitud y longitud, también da la fecha. Entonces hemos encontrado una API correcta. \n",
        "  \\n 2º - response.get()\n",
        "  \\n 3º - JSON\n",
        "  \\n 4º - date, location_id , lat, lng\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-w8Ptoi6tGIS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWV1HoXcr8V3",
        "colab_type": "text"
      },
      "source": [
        "### Ejercicio 2 \n",
        "Programad una función que retorne el estado meteorológico actual en una cierta localización, definida por su código postal (**zip code**) y código de país (e.g: us, uk, es, fr, etc). La función debe devolver una lista de tuplas de dos elementos, correspondientes al resumen del estado actual del tiempo **(weather.main)** y a la descripción extendida **(weather.description)**. Utilizad la API de [openweathermap](https://openweathermap.org/api) para obtener las predicciones.<span style=\"font-family: Courier New; background-color: #82b74b; color: #000000; padding: 3px; \">NM</span>\n",
        "\n",
        "Para utilizar la API necesitareis registraros y obtener una API key. Podéis registraros [aquí](https://home.openweathermap.org/users/sign_up) y obtener vuestra API key [aquí](https://home.openweathermap.org/api_keys) una vez registrados. Tened en cuenta que la API key puede tardar un rato en funcionar después de registraros, y la API os devolverá un error 401 conforme la clave no es valida:\n",
        "\n",
        "`{\"cod\":401, \"message\": \"Invalid API key. Please see http://openweathermap.org/faq#error401 for more info.\"}`\n",
        "\n",
        "Simplemente esperad un rato antes de utilizar la clave.\n",
        "\n",
        "**Hints**: \n",
        "\n",
        "- Veréis que en general la API esta documentada sin incluir la API key, aun que esta es necesaria. Deberéis incluir la API key en la llamada como uno de los parámetros de la URL (&appid=your_api_key):\n",
        "\n",
        "    http://example_url.com?param1=value1&param2=value2&appid=your_api_key\n",
        "\n",
        "- Os animamos a que paséis por el proceso de registro para que veáis de que trata y cómo se generan las API keys. Aún así, os proporcionamos una API key en caso de que tengáis problemas con el proceso.\n",
        "\n",
        "    owm_api_key = 'd54f26dbcf6d4136bc0ef8ba5f07825b'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcRy45vBuSG1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cbadfd68-78d6-464c-99ae-0c52250c00bc"
      },
      "source": [
        "import json \n",
        "import requests\n",
        "\n",
        "def parse_response(response):\n",
        "  data = None\n",
        "  if response.status_code == 200:\n",
        "    data = json.loads(response.content)\n",
        "  else: \n",
        "    raise Exception(\"Unexpected response (%s: %s).\" %(response.status_code, response.reason))\n",
        "  return data\n",
        "\n",
        "def get_weather_zip(zip_code, country, api_key ):\n",
        "  base_url = \"http://api.openweathermap.org/data/2.5/weather?zip=%s,%s&appid=%s\"\n",
        "  response = requests.get(base_url % (zip_code, country, api_key))\n",
        "  data = parse_response(response)\n",
        "  if data: \n",
        "    weather = data.get(\"weather\")\n",
        "    r = [(w.get(\"main\"), w.get(\"description\")) for w in weather]\n",
        "  else:\n",
        "    raise Exception(\"Couldn't get weather data\")\n",
        "  return r\n",
        "\n",
        "api_key = 'd54f26dbcf6d4136bc0ef8ba5f07825b' #Poner nuestra key\n",
        "zip_code = \"28030\"\n",
        "country_code = \"es\"\n",
        "weather_data = get_weather_zip(zip_code, country_code, api_key)\n",
        "\n",
        "print(weather_data)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Rain', 'moderate rain'), ('Mist', 'mist')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5--2gmAr8V7",
        "colab_type": "text"
      },
      "source": [
        "### Ejercicio 3\n",
        "\n",
        "[CoinMarketCap](https://coinmarketcap.com/) es una web con contenido acerca de\n",
        "las 100 criptomonedas con más capitalización de mercado.Programad un _crawler_ que extraiga los nombres y la capitalización de todas les monedas que se muestran en CoinMarketCap. Para hacerlo, utilizad la estructura de _crawler_ que hemos visto en el Notebook de esta unidad **modificando únicamente dos lineas de código**:\n",
        "\n",
        "- URL de inicio.\n",
        "- La expresión XPath que selecciona el contenido a capturar.\n",
        "\n",
        "**Pista**: tal vez os puede ser de utilidad investigar sobre la scrapy shell y utilizarla para encontrar la expresión XPath que necesitas para resolver el ejercicio.\n",
        "\n",
        "**Nota**: si la ejecución del _crawler_ os devuelve un error `ReactorNotRestartable`, reiniciad el núcleo del Notebook (en el menú: `Kernel` - `Restart`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHS3NSl8r8V8",
        "colab_type": "text"
      },
      "source": [
        "**Respuesta**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mY5cxQ89uYhw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "52f48a5f-5547-4a9a-89b5-8ddd546055cb"
      },
      "source": [
        "# Importamos scrapy.\n",
        "import scrapy\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "\n",
        "# Creamos la araña.\n",
        "class spiderCoinMarket(scrapy.Spider):\n",
        "    \n",
        "    # Asignamos un nombre a la araña.\n",
        "    name = \"spiderCoinMarket\"\n",
        "    \n",
        "    # Indicamos la url que queremos analizar en primer lugar.\n",
        "    start_urls = [\n",
        "        \"https://coinmarketcap.com/\"\n",
        "    ]\n",
        "\n",
        "    # Definimos el analizador.\n",
        "    def parse(self, response):\n",
        "        # Extraemos el título de las monedas y la capitalización\n",
        "        for currency in response.xpath('//td[@class=\"no-wrap currency-name\"]/@dat-sort'):\n",
        "            yield {\n",
        "                'currency': currency.extract()\n",
        "            }\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Creamos un crawler.\n",
        "    process = CrawlerProcess({\n",
        "        'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n",
        "        'DOWNLOAD_HANDLERS': {'s3': None},\n",
        "        'LOG_ENABLED': True\n",
        "    })\n",
        "\n",
        "    # Inicializamos el crawler con nuestra araña.\n",
        "    process.crawl(spiderCoinMarket)\n",
        "    \n",
        "    # Lanzamos la araña.\n",
        "    process.start()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-03-31 17:02:07 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapybot)\n",
            "2020-03-31 17:02:07 [scrapy.utils.log] INFO: Versions: lxml 4.2.6.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.6.9 (default, Nov  7 2019, 10:44:02) - [GCC 8.3.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Linux-4.14.137+-x86_64-with-Ubuntu-18.04-bionic\n",
            "2020-03-31 17:02:07 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
            "2020-03-31 17:02:07 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
            "2020-03-31 17:02:07 [scrapy.extensions.telnet] INFO: Telnet Password: c2fe1e3d762004f8\n",
            "2020-03-31 17:02:07 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2020-03-31 17:02:07 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2020-03-31 17:02:07 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2020-03-31 17:02:07 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2020-03-31 17:02:07 [scrapy.core.engine] INFO: Spider opened\n",
            "2020-03-31 17:02:07 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2020-03-31 17:02:07 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2020-03-31 17:02:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://coinmarketcap.com/> (referer: None)\n",
            "2020-03-31 17:02:07 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2020-03-31 17:02:07 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 233,\n",
            " 'downloader/request_count': 1,\n",
            " 'downloader/request_method_count/GET': 1,\n",
            " 'downloader/response_bytes': 83590,\n",
            " 'downloader/response_count': 1,\n",
            " 'downloader/response_status_count/200': 1,\n",
            " 'elapsed_time_seconds': 0.264914,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2020, 3, 31, 17, 2, 7, 510182),\n",
            " 'log_count/DEBUG': 1,\n",
            " 'log_count/INFO': 10,\n",
            " 'memusage/max': 180555776,\n",
            " 'memusage/startup': 180555776,\n",
            " 'response_received_count': 1,\n",
            " 'scheduler/dequeued': 1,\n",
            " 'scheduler/dequeued/memory': 1,\n",
            " 'scheduler/enqueued': 1,\n",
            " 'scheduler/enqueued/memory': 1,\n",
            " 'start_time': datetime.datetime(2020, 3, 31, 17, 2, 7, 245268)}\n",
            "2020-03-31 17:02:07 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}